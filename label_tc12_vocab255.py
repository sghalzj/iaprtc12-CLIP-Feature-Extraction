import os, re
from collections import Counter, defaultdict
import numpy as np
import scipy.io as sio
import xml.etree.ElementTree as ET

# === é…ç½®å‚æ•° ===
ANN_DIR = r"S:\dataset\iaprtc12\unpack\iaprtc12\annotations_complete_eng"
OUT_DIR = r"S:\SCH-main"
USE_STOPWORDS = True
MIN_DF = 5
TOP_K = 255
EXT_FILTER = {'.eng'}

DEFAULT_STOPWORDS = {
    # å† è¯ (Articles)
    "a", "an", "the",

    # æœ€å¸¸ç”¨ä»£è¯ (Pronouns)
    "i", "me", "my", "myself", "we", "us", "our", "ours", "ourselves",
    "you", "your", "yours", "yourself", "yourselves",
    "he", "him", "his", "himself", "she", "her", "hers", "herself",
    "it", "its", "itself", "they", "them", "their", "theirs", "themselves",

    # æŒ‡ç¤ºè¯å’Œä¸å®šä»£è¯ (Demonstratives & Indefinite Pronouns)
    "this", "that", "these", "those",
    "what", "which", "who", "whom", "whose",
    "other", "another", "same", "such",

    # ä»‹è¯ (Prepositions)
    "about", "above", "across", "after", "against", "along", "among", "around", "at", "before",
    "behind", "below", "beneath", "beside", "between", "beyond", "by", "down", "during", "for",
    "from", "in", "inside", "into", "near", "of", "off", "on", "onto", "out", "outside", "over",
    "through", "throughout", "to", "toward", "under", "until", "up", "upon", "with", "within", "without",

    # è¿è¯ (Conjunctions)
    "and", "or", "but", "nor", "so", "yet",
    "if", "because", "although", "since", "unless", "while", "where", "when", "as",

    # åŠ¨è¯ (Verbs)
    "is", "are", "was", "were", "be", "being", "been",
    "have", "has", "had", "having", "do", "does", "did", "doing",
    "can", "could", "will", "would", "shall", "should", "may", "might", "must",

    # å‰¯è¯ (Adverbs)
    "very", "really", "quite", "too", "just", "only", "also", "well", "much", "more", "most",
    "even", "still", "almost", "enough", "however", "therefore", "thus", "hence",
    "now", "then", "here", "there", "when", "where", "why", "how",

    # é™å®šè¯å’Œæ•°é‡è¯ (Determiners & Quantifiers)
    "all", "any", "each", "every", "no", "none", "some", "lot", "few", "many", "several", "both", "either", "neither",
    "own", "same", "so", "than", "too",

    # æ„Ÿå¹è¯å’Œå…¶ä»– (Interjections & Others)
    "oh", "yes", "no", "not", "ain", "aren", "couldn", "didn", "doesn", "don", "hadn", "hasn",
    "haven", "isn", "mightn", "mustn", "needn", "shan", "shouldn", "wasn", "weren", "won", "wouldn",

    # å¸¸è§è¯æ±‡
    "get", "got", "go", "goes", "going", "went", "come", "came", "see", "saw", "say", "said",
    "know", "knew", "think", "thought", "take", "took", "make", "made", "use", "used",
    "team",  # ä¿ç•™ï¼Œå› ä¸ºä¸åœ¨è¯è¯­åˆ—è¡¨ä¸­
    "brown", "blue", "yellow", "pink",  # ä¿ç•™è¿™äº›é¢œè‰²ï¼Œå› ä¸ºå®ƒä»¬ä¸åœ¨è¯è¯­åˆ—è¡¨ä¸­
    "next", "behind", "front", "left", "right",  # ä¿ç•™è¿™äº›æ–¹ä½è¯
    "high", "low", "broad", "narrow",
    "up", "down", "out", "off", "around", "some", "many", "few",
    "several", "both", "large", "small", "long", "short", "steep",
    "flat", "little", "big",  # ç§»é™¤äº†é‡å¤çš„ "high", "low"
    "flowers", "leaves",  # ä¿ç•™ï¼Œä½¿ç”¨å¤æ•°å½¢å¼

    # æ•°å­—å•è¯
    "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",

    # åŠ¨ä½œçŠ¶æ€è¯ï¼ˆç§»é™¤äº†åœ¨è¯è¯­åˆ—è¡¨ä¸­å‡ºç°çš„ï¼‰
    "posing", "walking", "lying", "dancing", "eating",
    "surrounded", "covered", "illuminated", "burning",

    # ç§»é™¤äº†ä»¥ä¸‹è¯è¯­ï¼ˆå› ä¸ºå®ƒä»¬å­˜åœ¨äºè¯è¯­åˆ—è¡¨ä¸­ï¼‰ï¼š
    # "back", "bench", "chair", "green", "grey", "orange", "red", "white", "table",
    # "rock", "people", "person", "man", "woman", "group", "life", "light", "power",
    # "side", "playing", "racing", "riding", "cycling", "holding", "standing", "sitting",
    # "middle", "rise" (è¿™äº›ä¹Ÿåœ¨ä¹‹å‰çš„åœç”¨è¯ä¸­ä½†å·²ç§»é™¤)
}



def norm_token(s: str) -> list:
    s = s.lower()
    # æ›´ç²¾ç¡®çš„æ–‡æœ¬æ¸…ç†
    s = re.sub(r"[^\w\s]", " ", s)  # ä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
    s = re.sub(r"\d+", " ", s)  # å»é™¤æ•°å­—
    s = re.sub(r"\s+", " ", s)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
    s = s.strip()

    tokens = s.split()
    if USE_STOPWORDS:
        tokens = [t for t in tokens if t not in DEFAULT_STOPWORDS and len(t) > 2]  # é¢å¤–è¿‡æ»¤çŸ­è¯
    return tokens

def read_text(fp, encs=("utf-8", "latin1", "cp1252")):
    for enc in encs:
        try:
            with open(fp, "r", encoding=enc, errors="ignore") as f:
                return f.read()
        except Exception:
            pass
    with open(fp, "rb") as f:
        return f.read().decode("latin1", "ignore")

def read_text(fp, encs=("utf-8", "latin1", "cp1252")):
    for enc in encs:
        try:
            with open(fp, "r", encoding=enc, errors="ignore") as f:
                return f.read()
        except Exception:
            pass
    with open(fp, "rb") as f:
        return f.read().decode("latin1", "ignore")

def collect_title_description_tokens(ann_dir):
    per_image_tokens = {}
    total_files = 0
    for root, _, files in os.walk(ann_dir):
        subdir = os.path.basename(root)
        for file in files:
            ext = os.path.splitext(file)[1].lower()
            if ext not in EXT_FILTER:
                continue
            base = os.path.splitext(file)[0]
            key = f"{subdir}/{base}"
            full_path = os.path.join(root, file)
            raw = read_text(full_path)

            try:
                tree = ET.fromstring(raw)
                # åªç”¨ DESCRIPTIONï¼Œä¸å†æ‹¼æ¥ TITLE
                desc = tree.findtext("DESCRIPTION", "")
                full_text = desc.strip()
                tokens = norm_token(full_text)
                if tokens:
                    per_image_tokens[key] = tokens
                    total_files += 1
            except ET.ParseError:
                print(f"âš ï¸ æ— æ³•è§£æï¼š{full_path}")
    print(f"å…±è¯»å– .eng æ–‡ä»¶: {total_files} ä¸ªï¼ˆæœ‰æ•ˆæ ·æœ¬ï¼‰")
    return per_image_tokens

def main():
    os.makedirs(OUT_DIR, exist_ok=True)

    print(f"è¯»å–æ³¨é‡Šç›®å½•ï¼š{ANN_DIR}")
    annotations = collect_title_description_tokens(ANN_DIR)
    keys = sorted(annotations.keys(), key=lambda x: [int(t) if t.isdigit() else t for t in re.findall(r'\d+|\D+', x)])
    N = len(keys)
    if N == 0:
        raise RuntimeError("âŒ æ²¡æœ‰è¯»å–åˆ°ä»»ä½•æœ‰æ•ˆæ ‡ç­¾ã€‚è¯·ç¡®è®¤æ–‡ä»¶ç»“æ„å’Œå†…å®¹æ ¼å¼ã€‚")

    # ç»Ÿè®¡è¯é¢‘
    freq = Counter(t for tags in annotations.values() for t in tags)
    if MIN_DF > 0:
        freq = Counter({w: c for w, c in freq.items() if c >= MIN_DF})
        if not freq:
            raise RuntimeError("âš ï¸ MIN_DF å¤ªé«˜ï¼Œè¯è¡¨ä¸ºç©ºï¼Œè¯·è°ƒæ•´å‚æ•°ã€‚")

    vocab = [w for w, _ in freq.most_common(TOP_K)]
    K = len(vocab)
    if K < TOP_K:
        print(f"[æç¤º] å®é™…è¯è¡¨å¤§å°ä»…ä¸º {K}ï¼Œä¸è¶³ {TOP_K}ã€‚")

    word2id = {w: i for i, w in enumerate(vocab)}

    # ç”Ÿæˆå¤šçƒ­æ ‡ç­¾
    Y = np.zeros((N, K), dtype=np.uint8)
    for i, k in enumerate(keys):
        for t in annotations[k]:
            j = word2id.get(t)
            if j is not None:
                Y[i, j] = 1

    # ä¿å­˜
    vocab_path = os.path.join(OUT_DIR, "iaprtc12_vocab255.txt")
    mat_path   = os.path.join(OUT_DIR, "iaprtc12_labels255.mat")
    npz_path   = os.path.join(OUT_DIR, "iaprtc12_labels255.npz")

    np.savetxt(vocab_path, np.array(vocab, dtype=str), fmt="%s", encoding="utf-8")
    sio.savemat(mat_path, {
        "keys":     np.array(keys, dtype=object),
        "vocab255": np.array(vocab, dtype=object),
        "Y_multi":  Y
    }, do_compression=True)
    np.savez_compressed(npz_path,
        keys=np.array(keys, dtype=object),
        vocab255=np.array(vocab, dtype=object),
        Y_multi=Y
    )

    density = float(Y.sum()) / (N * K)
    print("âœ… æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
    print(f"æ ·æœ¬æ•° N = {N}, æ ‡ç­¾ç»´åº¦ K = {K}, æ ‡ç­¾å¯†åº¦ = {density:.6f}")
    print("è¾“å‡ºæ–‡ä»¶ï¼š")
    print(" -", vocab_path)
    print(" -", mat_path)
    print(" -", npz_path)
    print("ğŸ“ å¯¹é½è¯´æ˜ï¼šå›¾åƒè·¯å¾„ä¸º images/{keys[i]}.jpg")

if __name__ == "__main__":
    main()
