import os, re
from collections import Counter, defaultdict
import numpy as np
import scipy.io as sio
import xml.etree.ElementTree as ET

# === é…ç½®å‚æ•° ===
ANN_DIR = r"S:\dataset\iaprtc12\unpack\iaprtc12\annotations_complete_eng"
OUT_DIR = r"S:\SCH-main"
USE_STOPWORDS = True
MIN_DF = 5
TOP_K = 255
EXT_FILTER = {'.eng'}

DEFAULT_STOPWORDS = {
    # ===== æ ¸å¿ƒè¯­æ³•è¯ï¼ˆå¿…é¡»è¿‡æ»¤ï¼‰=====
    # å† è¯
    "a", "an", "the",

    # ä»£è¯
    "i", "me", "my", "myself", "we", "us", "our", "ours", "ourselves",
    "you", "your", "yours", "yourself", "yourselves",
    "he", "him", "his", "himself", "she", "her", "hers", "herself",
    "it", "its", "itself", "they", "them", "their", "theirs", "themselves",

    # åŸºç¡€ç³»åŠ¨è¯å’ŒåŠ©åŠ¨è¯
    "is", "are", "was", "were", "be", "being", "been",
    "have", "has", "had", "having", "do", "does", "did", "doing",
    "can", "could", "will", "would", "shall", "should", "may", "might", "must",

    # åŸºæœ¬ä»‹è¯å’Œè¿è¯
    "and", "or", "but", "nor", "so", "yet",
    "if", "because", "although", "since", "unless", "while", "where", "when", "as",
    "about", "above", "across", "after", "against", "along", "among", "around", "at", "before",
    "behind", "below", "beneath", "beside", "between", "beyond", "by", "down", "during", "for",
    "from", "in", "inside", "into", "near", "of", "off", "on", "onto", "out", "outside", "over",
    "through", "throughout", "to", "toward", "under", "until", "up", "upon", "with", "within", "without",

    # ===== é™å®šè¯å’ŒæŠ½è±¡è¯ =====
    "all", "any", "each", "every", "no", "none", "some", "lot", "few", "many", "several", "both", "either", "neither",
    "own", "same", "so", "than", "too",

    # ===== æ„Ÿå¹è¯å’Œå¦å®šè¯ =====
    "oh", "yes", "no", "not", "ain", "aren", "couldn", "didn", "doesn", "don", "hadn", "hasn",
    "haven", "isn", "mightn", "mustn", "needn", "shan", "shouldn", "wasn", "weren", "won", "wouldn",

    # ===== è¿‡äºæ¨¡ç³Šçš„å½¢å®¹è¯å‰¯è¯ =====
    "very", "really", "quite", "too", "just", "only", "also", "well", "much", "more", "most",
    "even", "still", "almost", "enough", "however", "therefore", "thus", "hence",
    "now", "then", "here", "there", "why", "how",

    # ===== IAPRTC12ç‰¹å®šè¿‡æ»¤ =====
    # è¿™äº›è¯åœ¨æ•°æ®é›†ä¸­å‡ºç°é¢‘ç¹ä½†ä¿¡æ¯é‡ä½
    "image", "images", "photo", "photos", "picture", "pictures",
    "show", "shows", "shown", "showing", "see", "seen",
    "look", "looks", "looking", "view", "views", "viewing",

    # é€šç”¨åŠ¨è¯ï¼ˆåœ¨æ ‡æ³¨ä¸­ä¿¡æ¯é‡è¾ƒä½ï¼‰
    "get", "got", "go", "goes", "going", "went", "come", "came",
    "take", "took", "make", "made", "use", "used", "know", "knew",
    "think", "thought", "say", "said", "see", "saw"
    # å¸¸è§è¯æ±‡
    "get", "got", "go", "goes", "going", "went", "come", "came", "see", "saw", "say", "said",
    "know", "knew", "think", "thought", "take", "took", "make", "made", "use", "used",
    "team"

    "up", "down", "out", "off", "around", "some", "many", "few",
    "several", "both", "large", "small", "long", "short", "steep",
    "flat", "little", "big",  # ç§»é™¤äº†é‡å¤çš„ "high", "low"


    # æ•°å­—å•è¯
    "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",

}



def norm_token(s: str) -> list:
    s = s.lower()
    # æ›´ç²¾ç¡®çš„æ–‡æœ¬æ¸…ç†
    s = re.sub(r"[^\w\s]", " ", s)  # ä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
    s = re.sub(r"\d+", " ", s)  # å»é™¤æ•°å­—
    s = re.sub(r"\s+", " ", s)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
    s = s.strip()

    tokens = s.split()
    if USE_STOPWORDS:
        tokens = [t for t in tokens if t not in DEFAULT_STOPWORDS and len(t) > 2]  # é¢å¤–è¿‡æ»¤çŸ­è¯
    return tokens

def read_text(fp, encs=("utf-8", "latin1", "cp1252")):
    for enc in encs:
        try:
            with open(fp, "r", encoding=enc, errors="ignore") as f:
                return f.read()
        except Exception:
            pass
    with open(fp, "rb") as f:
        return f.read().decode("latin1", "ignore")

def read_text(fp, encs=("utf-8", "latin1", "cp1252")):
    for enc in encs:
        try:
            with open(fp, "r", encoding=enc, errors="ignore") as f:
                return f.read()
        except Exception:
            pass
    with open(fp, "rb") as f:
        return f.read().decode("latin1", "ignore")

def collect_title_description_tokens(ann_dir):
    per_image_tokens = {}
    total_files = 0
    for root, _, files in os.walk(ann_dir):
        subdir = os.path.basename(root)
        for file in files:
            ext = os.path.splitext(file)[1].lower()
            if ext not in EXT_FILTER:
                continue
            base = os.path.splitext(file)[0]
            key = f"{subdir}/{base}"
            full_path = os.path.join(root, file)
            raw = read_text(full_path)

            try:
                tree = ET.fromstring(raw)
                # åªç”¨ DESCRIPTIONï¼Œä¸å†æ‹¼æ¥ TITLE
                desc = tree.findtext("DESCRIPTION", "")
                full_text = desc.strip()
                tokens = norm_token(full_text)
                if tokens:
                    per_image_tokens[key] = tokens
                    total_files += 1
            except ET.ParseError:
                print(f"âš ï¸ æ— æ³•è§£æï¼š{full_path}")
    print(f"å…±è¯»å– .eng æ–‡ä»¶: {total_files} ä¸ªï¼ˆæœ‰æ•ˆæ ·æœ¬ï¼‰")
    return per_image_tokens

def main():
    os.makedirs(OUT_DIR, exist_ok=True)

    print(f"è¯»å–æ³¨é‡Šç›®å½•ï¼š{ANN_DIR}")
    annotations = collect_title_description_tokens(ANN_DIR)
    keys = sorted(annotations.keys(), key=lambda x: [int(t) if t.isdigit() else t for t in re.findall(r'\d+|\D+', x)])
    N = len(keys)
    if N == 0:
        raise RuntimeError("âŒ æ²¡æœ‰è¯»å–åˆ°ä»»ä½•æœ‰æ•ˆæ ‡ç­¾ã€‚è¯·ç¡®è®¤æ–‡ä»¶ç»“æ„å’Œå†…å®¹æ ¼å¼ã€‚")

    # ç»Ÿè®¡è¯é¢‘
    freq = Counter(t for tags in annotations.values() for t in tags)
    if MIN_DF > 0:
        freq = Counter({w: c for w, c in freq.items() if c >= MIN_DF})
        if not freq:
            raise RuntimeError("âš ï¸ MIN_DF å¤ªé«˜ï¼Œè¯è¡¨ä¸ºç©ºï¼Œè¯·è°ƒæ•´å‚æ•°ã€‚")

    vocab = [w for w, _ in freq.most_common(TOP_K)]
    K = len(vocab)
    if K < TOP_K:
        print(f"[æç¤º] å®é™…è¯è¡¨å¤§å°ä»…ä¸º {K}ï¼Œä¸è¶³ {TOP_K}ã€‚")

    word2id = {w: i for i, w in enumerate(vocab)}

    # ç”Ÿæˆå¤šçƒ­æ ‡ç­¾
    Y = np.zeros((N, K), dtype=np.uint8)
    for i, k in enumerate(keys):
        for t in annotations[k]:
            j = word2id.get(t)
            if j is not None:
                Y[i, j] = 1

    # ä¿å­˜
    vocab_path = os.path.join(OUT_DIR, "iaprtc12_vocab255.txt")
    mat_path   = os.path.join(OUT_DIR, "iaprtc12_labels255.mat")
    npz_path   = os.path.join(OUT_DIR, "iaprtc12_labels255.npz")

    np.savetxt(vocab_path, np.array(vocab, dtype=str), fmt="%s", encoding="utf-8")
    sio.savemat(mat_path, {
        "keys":     np.array(keys, dtype=object),
        "vocab255": np.array(vocab, dtype=object),
        "Y_multi":  Y
    }, do_compression=True)
    np.savez_compressed(npz_path,
        keys=np.array(keys, dtype=object),
        vocab255=np.array(vocab, dtype=object),
        Y_multi=Y
    )

    density = float(Y.sum()) / (N * K)
    print("âœ… æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
    print(f"æ ·æœ¬æ•° N = {N}, æ ‡ç­¾ç»´åº¦ K = {K}, æ ‡ç­¾å¯†åº¦ = {density:.6f}")
    print("è¾“å‡ºæ–‡ä»¶ï¼š")
    print(" -", vocab_path)
    print(" -", mat_path)
    print(" -", npz_path)
    print("ğŸ“ å¯¹é½è¯´æ˜ï¼šå›¾åƒè·¯å¾„ä¸º images/{keys[i]}.jpg")

if __name__ == "__main__":
    main()
